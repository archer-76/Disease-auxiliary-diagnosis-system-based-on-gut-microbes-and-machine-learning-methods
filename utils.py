from cmath import nan
from operator import index
import string
from typing import List
from xmlrpc.client import Boolean
import numpy as np
import pandas as pd
import torch
from scipy.sparse import coo_matrix
from sklearn.preprocessing import LabelEncoder
from torch_geometric.data import Data
from torch_geometric.loader import DenseDataLoader
# load dataset
from torch_geometric.datasets import TUDataset
import torch_geometric.transforms as T


def get_benchmark_dataset(dataset_name='ENZYMES'):
    dataset = TUDataset('data/TUDataset', name=dataset_name)
    dataset = dataset.shuffle()
    datalist = []
    for i in range(len(dataset)):
        data = dataset[i]
        data = Data(x=data.x, y=data.y, edge_index=data.edge_index)
        datalist.append(data)
    print(type(datalist[0]), datalist[0])
    return datalist


def get_dataset(smpl_path: str, muti_target: Boolean = False):
    """Generate dataset from csv files.

    Args:
        smpl_path (str):
            Path of samples file.

        muti_target(Boolean):
            use multiple target or not
        y:
            Adjenct matrix of samples with shape of samples_num, samples_num
            y is generated by MENA, involving tons of diseased and healthy samples
    Returns:
        List: list of Data(x, edge_index) every data is a sub_graph.
    """
    samples_df = pd.read_csv(smpl_path, header=None, dtype={"disease": str})
    # keep only the rows with at least n non-zero values
    samples_df = samples_df.replace(0, np.nan)
    samples_df = samples_df.dropna(thresh=50)
    samples_df = samples_df.replace(np.nan, 0)
    y_list = samples_df.iloc[0, 1:].to_numpy()
    le = LabelEncoder()
    le = le.fit(['n', 't2d', 'obesity', 'ibd', 'adenoma', 'cirrhosis'])
    print(le.classes_)
    # le = le.fit(['leaness', 'obesity'])
    y_list = le.transform(y_list).reshape(1, -1)
    # 按照首字母排序，n的下标为3，第四个，如果不是多分类任务，设置n为0，其他为1
    if (not muti_target):
        y_list = np.where(y_list == 3, 0, 1)
    print(len(y_list[0]), np.count_nonzero(y_list))
    samples = samples_df.iloc[1:, 1:].to_numpy(dtype=np.float32)
    assert (y_list.shape[1] == samples.shape[1])
    print('sample.shape[0]', samples.shape[0])
    giant_edge_index, adj = construct_adj(samples)
    print('giant_edge_index.shape', giant_edge_index.shape)
    data_list = []
    for i in range(samples.shape[1]):
        x = samples[:, i]
        y = y_list[:, i]
        edge_index = get_edge_index(x, adj, method="index")
        x = x.reshape((x.shape[0], 1))
        x = torch.as_tensor(x, dtype=torch.float32)
        y = torch.as_tensor(y, dtype=int)
        assert (x.dim() == 2)
        reversed_edge_index = torch.vstack(
            (edge_index[1, :], edge_index[0, :]))
        edge_index = torch.hstack((edge_index, reversed_edge_index))
        dense_adj = torch.sparse_coo_tensor(
            edge_index, torch.ones(edge_index.shape[1]),
            torch.Size([x.shape[0], x.shape[0]]))
        dense_adj = dense_adj.to_dense()
        adj = dense_adj.fill_diagonal_(1.)
        data = Data(x=x, y=y, adj=adj, edge_index=edge_index)
        data_list.append(data)

    return data_list


def get_edge_index(x: np.ndarray, y: np.ndarray, method: str):
    """return adjenct matrix of x by looking up in y, the giant adjenct matrix.

    Args:
        x:
            a row of the OTU table, represent to a single sample, a Kranker.
        y:
            Adjenct matrix of samples with shape of samples_num, samples_num
            y is generate by MENA, involved tons of diseased and healthy samples
        method:
            if index:
                generate the edge_index by index

    Returns:
        edge_index: shape of (2,interactions)
    """
    if method == 'index':
        index = np.nonzero(x)
        adj = y[index, index]
        adj_sparse = coo_matrix(adj)
        adj_indices = np.vstack((adj_sparse.row, adj_sparse.col))
        edge_index = torch.LongTensor(adj_indices)
    return edge_index


def get_dataset_for_MENA(smpl_path: str, flag=True):
    """pre_process the csv files.

    Args:
        smpl_path (str): Path of samples file.

    Returns:
        no returns, directly change the file.
    """
    samples_df = pd.read_csv('./data/samples.csv')
    if flag == True:
        otus = samples_df.iloc[1:, :].to_numpy()
        otu_figures = otus[:, 1:]
        otu_figures = np.where(otu_figures < np.nanmin(otu_figures) * 100,
                               np.NaN, otu_figures)
        otu_index = np.arange(int(otu_figures.shape[1] / 2))
        np.random.shuffle(otu_index)
        print(otu_index.shape)
        otu_index = otu_index.reshape((otu_index.shape[0], -1))
        otu_figures = otu_figures[:, otu_index]
        print(otu_figures.shape)
        otu_figures = otu_figures.reshape((otu_figures.shape[0], -1))
        otu_figures *= (1. / np.nanmin(otu_figures))
        otus = np.hstack(((1 + np.asarray(range(otus.shape[0]))).reshape(
            (-1, 1)), otu_figures + 0.01))
        df = pd.DataFrame(otus,
                          columns=['name'] +
                          [f's{i+1}' for i in range(otus.shape[1] - 1)])

    else:
        df = samples_df.dropna(thresh=15)
    df.to_csv('./data/giant_matrix.csv', index=False)


# 在测试后，cutoff为0.62，0.64，0.68的时候卡方检验泊松分布转化完成
# p-value分别为0.05,0.01,0.001，此x必须是t2d数据集
# 如果更换数据集，请重新使用MENA软件测试http://ieg4.rccc.ou.edu/mena/
# 对于giant_matrix多次检验结果分别是0.66，0.68，0.69
def construct_adj(x: np.ndarray, score_thresh=0.34):
    """construct the enormous adjenct matrix.

    Args:
        x (np.ndarray): the OTU used to calculate corrcoef score.
        score_thresh  : from MENA online website to cut off
    Returns:
        edge_index: shape of (2, num_of_cor_pairs)
        adj : shape of (x.shape[0],x.shape[0])
    """
    x = x * 10
    x = np.where(x == 0, 0.01, x)
    adj = np.corrcoef(x)
    adj = np.multiply((np.abs(adj) > score_thresh), adj)
    adj_sparse = coo_matrix(adj)
    adj_indices = np.vstack((adj_sparse.row, adj_sparse.col))
    edge_index = torch.LongTensor(adj_indices)
    return edge_index, adj

    # get_dataset_for_MENA(a, False)


class CollateFn:
    def __init__(self, device='cpu'):
        self.device = device

    def __call__(self, batch):
        adj_tensor_list = []
        features_list = []
        mask_list = []
        # (adj, features), labels = list(zip(*batch))
        max_num_nodes = max([g[0][0].shape[0] for g in batch])
        labels = []
        for (A, F), L in batch:
            labels.append(L)
            length = A.shape[0]
            pad_len = max_num_nodes - length
            adj_tensor_list.append(
                np.pad(A, ((0, pad_len), (0, pad_len)), mode='constant'))
            features_list.append(
                np.pad(F, ((0, pad_len), (0, 0)), mode='constant'))
            mask = np.zeros(max_num_nodes)
            mask[:length] = 1
            mask_list.append(mask)
        return torch.from_numpy(np.stack(adj_tensor_list, 0)).float().to(self.device), \
               torch.from_numpy(np.stack(features_list, 0)).float().to(self.device), \
               torch.from_numpy(np.stack(mask_list, 0)).float().to(self.device), \
               torch.from_numpy(np.stack(labels, 0)).long().to(self.device)